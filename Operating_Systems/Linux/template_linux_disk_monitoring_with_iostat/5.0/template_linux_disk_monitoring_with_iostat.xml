<?xml version="1.0" encoding="UTF-8"?>
<zabbix_export>
  <version>5.0</version>
  <date>2021-11-09T04:50:54Z</date>
  <groups>
    <group>
      <name>Halley template</name>
    </group>
  </groups>
  <templates>
    <template>
      <template>Halley Linux Disk_iostat</template>
      <name>Halley Linux Disk_iostat</name>
      <description>## Overview Template for monitoring Linux Disk with iostat with explanations and advices. Use LLD discovery: discover only [hsv]d[a-z] The idea is from &lt;https://wiki.enchtex.info/howto/zabbix/zabbix_iostat_monitoring> I edited the script and add explanations and advises to the template Script write data in a cache file to not make a lot of requests To install: follow instructions from &lt;https://cloud.mail.ru/public/6LpD/qkVgL4E8z> Any recommends are welcome ## Author Tudor Ticau</description>
      <groups>
        <group>
          <name>Halley template</name>
        </group>
      </groups>
      <applications>
        <application>
          <name>Iostat</name>
        </application>
      </applications>
      <discovery_rules>
        <discovery_rule>
          <name>IOSTAT: Hard disk drive discovery</name>
          <key>iostat[]</key>
          <delay>60</delay>
          <lifetime>1d</lifetime>
          <item_prototypes>
            <item_prototype>
              <name>iostat: Avg queue length on $2</name>
              <key>iostat[avgqu,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>req</units>
              <description>The average queue length of the requests that were issued to the device. - Average Queue Lenght. Usual is 2 per physical disk. So, if we have 5 disks, an average queue lenght under 10 is fine</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
            </item_prototype>
            <item_prototype>
              <name>iostat: Avg queue size on $2</name>
              <key>iostat[avgrq,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>B</units>
              <description>The average queue size of the requests that were issued to the device. Is calculated in sectors*512 (usual size of a sector in Unix)</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
              <preprocessing>
                <step>
                  <type>MULTIPLIER</type>
                  <params>512</params>
                </step>
              </preprocessing>
            </item_prototype>
            <item_prototype>
              <name>iostat: I/O latency on $2</name>
              <key>iostat[await,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>ms</units>
              <description>The average time (in milliseconds) for I/O requests issued to the device to be served. This includes the time spent by the requests in queue and the time spent servicing them. In other words I/O time for complete a process (read the disk, change data, write in disk) - (SSD usually is under 5 ms, HDD 100 ms )</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
              <trigger_prototypes>
                <trigger_prototype>
                  <expression>{avg(120)}>200</expression>
                  <name>Disk: {#HDNAME} I/O latency high</name>
                  <priority>AVERAGE</priority>
                  <description>Disk transfer time for an I/O operations take over 200 ms to execute. This can be a problem for your disk performance. Consider that an SSD must complete an I/O req in &lt; 5 ms, and a HDD &lt; 150 ms</description>
                  <manual_close>YES</manual_close>
                  <tags>
                    <tag>
                      <tag>Value</tag>
                      <value>{ITEM.VALUE}</value>
                    </tag>
                  </tags>
                </trigger_prototype>
              </trigger_prototypes>
            </item_prototype>
            <item_prototype>
              <name>iostat: Read I/O latency on $2</name>
              <key>iostat[rawait,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>ms</units>
              <description>The average time (in milliseconds) for read I/O requests issued to the device to be served. This includes the time spent by the requests in queue and the time spent servicing them. (SSD usually is under 1 ms, HDD 30 ms )</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
            </item_prototype>
            <item_prototype>
              <name>iostat: KBps $1 on $2</name>
              <key>iostat[read,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>Bps</units>
              <description>The total number of kilobytes read per second. SSD has until 400 Mb/s, HDD until 60 Mb/s</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
              <preprocessing>
                <step>
                  <type>CHANGE_PER_SECOND</type>
                  <params></params>
                </step>
                <step>
                  <type>MULTIPLIER</type>
                  <params>1024</params>
                </step>
              </preprocessing>
            </item_prototype>
            <item_prototype>
              <name>iostat: Merged read IOPS on $2</name>
              <key>iostat[rrqm,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>req/s</units>
              <description>The number of read requests merged per second that were queued to the device. Reads and writes which are adjacent to each other may be merged for efficiency. Thus two 4K reads may become one 8K read before it is ultimately handed to the disk, and so it will be counted (and queued) as only one I/O. This field lets you know how often this was done.</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
            </item_prototype>
            <item_prototype>
              <name>iostat: Read IOPS on $2</name>
              <key>iostat[rs,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>req</units>
              <description>The number of read requests that were issued to the device per second. In other words, how many IOPS per second are read. SSD can handle until 30 000 IOPS, HDD - 170</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
            </item_prototype>
            <item_prototype>
              <name>iostat: Utilization % on $2</name>
              <key>iostat[util,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>%</units>
              <description>Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100%.</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
              <trigger_prototypes>
                <trigger_prototype>
                  <expression>{avg(120)}>90</expression>
                  <name>Disk: {#HDNAME} Utilization high</name>
                  <priority>HIGH</priority>
                  <description>Saturation (bandwidth utilization for the device). Device saturation occurs when this value is close to 100% and in this time device can't deserve CPU requests. Server is going very slow</description>
                  <manual_close>YES</manual_close>
                  <tags>
                    <tag>
                      <tag>Value</tag>
                      <value>{ITEM.VALUE}</value>
                    </tag>
                  </tags>
                </trigger_prototype>
              </trigger_prototypes>
            </item_prototype>
            <item_prototype>
              <name>iostat: Write I/O latency on $2</name>
              <key>iostat[wawait,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>ms</units>
              <description>The average time (in milliseconds) for write I/O requests issued to the device to be served. This includes the time spent by the requests in queue and the time spent servicing them. (SSD usually is under 4 ms, HDD 70 ms )</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
            </item_prototype>
            <item_prototype>
              <name>iostat: KBps $1 on $2</name>
              <key>iostat[write,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>Bps</units>
              <description>The total number of kilobytes written per second. SSD has until 400 Mb/s, HDD until 60 Mb/s</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
              <preprocessing>
                <step>
                  <type>CHANGE_PER_SECOND</type>
                  <params></params>
                </step>
                <step>
                  <type>MULTIPLIER</type>
                  <params>1024</params>
                </step>
              </preprocessing>
            </item_prototype>
            <item_prototype>
              <name>iostat: Merged write IOPS on $2</name>
              <key>iostat[wrqm,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>req/s</units>
              <description>The number of write requests merged per second that were queued to the device. Reads and writes which are adjacent to each other may be merged for efficiency. Thus two 4K reads may become one 8K read before it is ultimately handed to the disk, and so it will be counted (and queued) as only one I/O. This field lets you know how often this was done.</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
            </item_prototype>
            <item_prototype>
              <name>iostat: Write IOPS on $2</name>
              <key>iostat[ws,{#HDNAME}]</key>
              <delay>60</delay>
              <history>7d</history>
              <value_type>FLOAT</value_type>
              <units>req</units>
              <description>The number of write requests that were issued to the device per second. In other words, how many IOPS per second are read. SSD can handle until 10 000 IOPS, HDD - 170</description>
              <applications>
                <application>
                  <name>Iostat</name>
                </application>
              </applications>
            </item_prototype>
          </item_prototypes>
          <graph_prototypes>
            <graph_prototype>
              <name>iostat: {#HDNAME} Latency</name>
              <graph_items>
                <graph_item>
                  <color>1A7C11</color>
                  <item>
                    <host>Halley Linux Disk_iostat</host>
                    <key>iostat[await,{#HDNAME}]</key>
                  </item>
                </graph_item>
              </graph_items>
            </graph_prototype>
            <graph_prototype>
              <name>iostat: {#HDNAME} Traffic</name>
              <ymin_type_1>FIXED</ymin_type_1>
              <graph_items>
                <graph_item>
                  <color>009900</color>
                  <item>
                    <host>Halley Linux Disk_iostat</host>
                    <key>iostat[read,{#HDNAME}]</key>
                  </item>
                </graph_item>
                <graph_item>
                  <sortorder>1</sortorder>
                  <color>DD0000</color>
                  <item>
                    <host>Halley Linux Disk_iostat</host>
                    <key>iostat[write,{#HDNAME}]</key>
                  </item>
                </graph_item>
              </graph_items>
            </graph_prototype>
            <graph_prototype>
              <name>iostat: {#HDNAME} Utilization</name>
              <ymin_type_1>FIXED</ymin_type_1>
              <graph_items>
                <graph_item>
                  <color>00EEEE</color>
                  <item>
                    <host>Halley Linux Disk_iostat</host>
                    <key>iostat[util,{#HDNAME}]</key>
                  </item>
                </graph_item>
                <graph_item>
                  <sortorder>1</sortorder>
                  <color>00DD00</color>
                  <yaxisside>RIGHT</yaxisside>
                  <item>
                    <host>Halley Linux Disk_iostat</host>
                    <key>iostat[rs,{#HDNAME}]</key>
                  </item>
                </graph_item>
                <graph_item>
                  <sortorder>2</sortorder>
                  <color>FF6666</color>
                  <yaxisside>RIGHT</yaxisside>
                  <item>
                    <host>Halley Linux Disk_iostat</host>
                    <key>iostat[ws,{#HDNAME}]</key>
                  </item>
                </graph_item>
              </graph_items>
            </graph_prototype>
          </graph_prototypes>
        </discovery_rule>
      </discovery_rules>
    </template>
  </templates>
</zabbix_export>