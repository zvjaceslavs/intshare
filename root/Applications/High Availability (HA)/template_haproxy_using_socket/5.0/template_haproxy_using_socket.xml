<?xml version="1.0" encoding="UTF-8"?>
<zabbix_export><version>5.0</version><date>2021-10-19T06:16:14Z</date><groups><group><name>Halley template</name></group></groups><templates><template><template>HAProxy</template><name>HAProxy</name><groups><group><name>Halley template</name></group></groups><applications><application><name>HAProxy</name></application></applications><items><item><name>HAProxy memory used</name><key>proc.mem[haproxy]</key><delay>300</delay><value_type>FLOAT</value_type><units>b</units><applications><application><name>HAProxy</name></application></applications></item><item><name>HAProxy number of running processes</name><key>proc.num[haproxy]</key><delay>60</delay><applications><application><name>HAProxy</name></application></applications><triggers><trigger><expression>{last()}&lt;1</expression><name>HAProxy is not running on {HOST.NAME}</name><priority>HIGH</priority></trigger></triggers></item><item><name>HAProxy config file checksum ($1)</name><key>vfs.file.cksum[{$HAPROXY_CONFIG}]</key><delay>600</delay><applications><application><name>HAProxy</name></application></applications><triggers><trigger><expression>{diff(0)}&gt;0</expression><name>HAProxy config file changed on {HOST.NAME}</name><priority>WARNING</priority></trigger></triggers></item></items><discovery_rules><discovery_rule><name>HAProxy backend discovery</name><key>haproxy.list.discovery[{$HAPROXY_SOCK},BACK]</key><delay>1h</delay><lifetime>5d</lifetime><item_prototypes><item_prototype><name>HAProxy Backend [{#BACKEND_NAME}] bytes in</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,bin]</key><delay>60</delay><value_type>FLOAT</value_type><units>bps</units><description>HAProxy Backend bytes in</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Backend [{#BACKEND_NAME}] bytes out</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,bout]</key><delay>60</delay><value_type>FLOAT</value_type><units>bps</units><description>HAProxy Backend bytes out</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Backend [{#BACKEND_NAME}]: Responses denied per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,dresp]</key><delay>60</delay><status>DISABLED</status><description>Responses denied due to security concerns (ACL-restricted).&#13;
In most cases denials will originate in the frontend (e.g., a user is attempting to access an unauthorized URL). However, sometimes a request may be benign, yet the corresponding response contains sensitive information. In that case, you would want to set up an ACL to deny the offending response. Backend responses that are denied due to ACL restrictions will emit a 502 error code. With properly configured access controls on frontend, this metric should stay at or near zero.&#13;
Denied responses and an increase in 5xx responses go hand-in-hand. If you are seeing a large number of 5xx responses, you should check your denied responses to shed some light on the increase in error codes</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Backend [{#BACKEND_NAME}]: Errors connection per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,econ]</key><delay>60</delay><status>DISABLED</status><units>ms</units><description>Number of requests that encountered an error attempting to connect to a backend server.&#13;
Backend connection failures should be acted upon immediately. Unfortunately, the econ metric not only includes failed backend requests but additionally includes general backend errors, like a backend without an active frontend. Thankfully, correlating this metric with eresp and response codes from both frontend and backend servers will give a better idea of the causes of an increase in backend connection errors.</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Backend [{#BACKEND_NAME}] : Response errors per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,eresp]</key><delay>60</delay><status>DISABLED</status><description>Number of requests whose responses yielded an error&#13;
This represents the number of response errors generated by your backends. This includes errors caused by data transfers aborted by the servers as well as write errors on the client socket and failures due to ACLs. Combined with other error metrics, the backend error response rate helps diagnose the root cause of response errors. For example, an increase in both the backend error response rate and denied responses could indicate that clients are repeatedly attempting to access ACL-ed resources.</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;10</expression><name>HAProxy Backend [{#BACKEND_NAME}]: Number of responses with error is more than 10 for 5m</name><status>DISABLED</status><priority>AVERAGE</priority><description>Number of requests on backend, whose responses yielded an error, is more than 10.&#13;
The backend error response rate represents the number of response errors generated by your backends. This includes errors caused by data transfers aborted by the servers as well as write errors on the client socket and failures due to ACLs. Combined with other error metrics, the backend error response rate helps diagnose the root cause of response errors. For example, an increase in both the backend error response rate and denied responses could indicate that clients are repeatedly attempting to access ACL-ed resources.</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Backend [{#BACKEND_NAME}]: Unassigned requests</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,qcur]</key><delay>60</delay><status>DISABLED</status><description>Current number of requests unassigned in queue.&#13;
The qcur metric tracks the current number of connections awaiting assignment to a backend server. If you have enabled cookies and the listed server is unavailable, connections will be queued until the queue timeout is reached</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;10</expression><name>HAProxy Backend [{#BACKEND_NAME}]: Current number of requests unassigned in queue is more than  10 for 5m</name><status>DISABLED</status><priority>AVERAGE</priority><description>Current number of requests on backend unassigned in queue is more than 10.&#13;
If your backend is bombarded with connections to the point you have reached your global maxconn limit, HAProxy will seamlessly queue new connections in system kernelâ€™s socket queue until a backend server becomes available.&#13;
Keeping connections out of the queue is ideal, resulting in less latency and a better user experience. You should alert if the size of your queue exceeds the threshold. If you find that connections are consistently enqueueing, configuration changes may be in order, such as increasing global maxconn limit or changing the connection limits on individual backend servers.&#13;
Keep in mind: empty queue = happy client</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Backend [{#BACKEND_NAME}]: Time in queue</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,qtime]</key><delay>60</delay><units>s</units><description>Average time spent in queue (in ms) for the last 1,024 requests&#13;
Minimizing time spent in the queue results in lower latency and an overall better client experience. Each use case can tolerate a certain amount of queue time but in general, you should aim to keep this value as low as possible</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>MULTIPLIER</type><params>0.001</params></step></preprocessing><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;10s</expression><name>HAProxy Backend [{#BACKEND_NAME}]: Average time spent in queue is more than 10 sec for 5m</name><priority>AVERAGE</priority><description>Average time spent in queue (in ms) for the last 1,024 requests is more than 10 s.&#13;
It is obviously that minimizing time spent in the queue results in lower latency and an overall better client experience. Each use case can tolerate a certain amount of queue time but in general, you should aim to keep this value as low as possible</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Backend [{#BACKEND_NAME}]: Responses time</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,rtime]</key><delay>60</delay><value_type>FLOAT</value_type><units>s</units><description>Average backend response time (in ms) for the last 1,024 requests&#13;
Tracking average response times is an effective way to measure the latency of your load-balancing setup. Generally speaking, response times in excess of 500 ms will lead to degradation of application performance and customer experience. Monitoring the average response time can give you the upper hand to respond to latency issues before your customers are substantially impacted.&#13;
Keep in mind that this metric will be zero if you are not using HTTP (see #60)</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>MULTIPLIER</type><params>0.001</params></step></preprocessing><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;10s</expression><name>HAProxy Backend [{#BACKEND_NAME}]: Average response time is more than 10 sec for 5m</name><priority>AVERAGE</priority><description>Average backend response time (in ms) for the last 1,024 requests is more than 10 seconds.&#13;
Tracking average response times is an effective way to measure the latency of haproxy load-balancing setup. Generally speaking, response times in excess of 500 ms will lead to degradation of application performance and customer experience. Monitoring the average response time can give you the upper hand to respond to latency issues before your customers are substantially impacted.&#13;
Keep in mind that this metric will be zero if you are not using HTTP</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Backend  [{#BACKEND_NAME}]: Status</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,status]</key><delay>60</delay><description>HAProxy Backend  [{#BACKEND_NAME}] status&#13;
UP = 1&#13;
DOWN = 0</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><valuemap><name>Service state</name></valuemap><preprocessing><step><type>BOOL_TO_DECIMAL</type><params/></step></preprocessing><trigger_prototypes><trigger_prototype><expression>{max(#5)}=0</expression><name>HAProxy Backend [{#BACKEND_NAME}]: Server is DOWN</name><priority>DISASTER</priority><description>HAProxy Backend [{#BACKEND_NAME}] is not available.</description></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Backend [{#BACKEND_NAME}]: Redispatched requests per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,wredis]</key><delay>60</delay><status>DISABLED</status><units>conn</units><description>Number of times a request was redispatched to a different backend.&#13;
The redispatch rate metric tracks the number of times a client connection was unable to reach its original target, and was subsequently sent to a different server. If a client holds a cookie referencing a backend server that is down, the default action is to respond to the client with a 502 status code. However, if is  enabled option redispatch in haproxy.cfg, the request will be sent to any available backend server and the cookie will be ignored.</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Backend [{#BACKEND_NAME}]: Retried connections per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,wretr]</key><delay>60</delay><status>DISABLED</status><description>Number of times a connection was retried.&#13;
Some dropped or timed-out connections are to be expected when connecting to a backend server. The retry rate represents the number of times a connection to a backend server was retried. This metric is usually non-zero under normal operating conditions. Should you begin to see more retries than usual, it is likely that other metrics will also change, including econ and eresp.&#13;
Tracking the retry rate in addition to the above two error metrics can shine some light on the true cause of an increase in errors</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype></item_prototypes><graph_prototypes><graph_prototype><name>HAProxy Backend [{#BACKEND_NAME}] Redispatched requests and retried connections per second</name><graph_items><graph_item><color>1A7C11</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,wredis]</key></item></graph_item><graph_item><sortorder>1</sortorder><color>F63100</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,wretr]</key></item></graph_item></graph_items></graph_prototype><graph_prototype><name>HAProxy Backend [{#BACKEND_NAME}] Responses time and time in queue</name><graph_items><graph_item><color>1A7C11</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,rtime]</key></item></graph_item><graph_item><sortorder>1</sortorder><color>F63100</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,qtime]</key></item></graph_item></graph_items></graph_prototype><graph_prototype><name>HAProxy Backend [{#BACKEND_NAME}] Status</name><graph_items><graph_item><color>1A7C11</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,status]</key></item></graph_item></graph_items></graph_prototype><graph_prototype><name>HAProxy Backend [{#BACKEND_NAME}] Traffic</name><graph_items><graph_item><color>1A7C11</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,bin]</key></item></graph_item><graph_item><sortorder>1</sortorder><color>F63100</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},BACKEND,bout]</key></item></graph_item></graph_items></graph_prototype></graph_prototypes></discovery_rule><discovery_rule><name>HAProxy frontend discovery</name><key>haproxy.list.discovery[{$HAPROXY_SOCK},FRONT]</key><delay>1d</delay><lifetime>5d</lifetime><item_prototypes><item_prototype><name>HAProxy Frontend  [{#FRONTEND_NAME}]: Incoming traffic</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,bin]</key><delay>60</delay><value_type>FLOAT</value_type><units>bps</units><description>Number of bits received by the frontend</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>MULTIPLIER</type><params>8</params></step><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Outgoing traffic</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,bout]</key><delay>60</delay><units>bps</units><description>Number of bits sent by the frontend</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step><step><type>MULTIPLIER</type><params>8</params></step></preprocessing></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Denied requests per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,dreq]</key><delay>60</delay><description>Requests denied due to security concerns (ACL-restricted) per second.&#13;
An increase in denied requests will subsequently cause an increase in 403 Forbidden codes. &#13;
- For tcp this is because of a matched tcp-request content rule.&#13;
- For http this is because of a matched http-request or tarpit rule.&#13;
Correlating the two can help to discern the root cause of an increase in 4xx responses.</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;10</expression><name>HAProxy Frontend [{#FRONTEND_NAME}]: Number of requests denied is more than 10  for 5m</name><priority>AVERAGE</priority><description>Number of requests denied due to security concerns (ACL-restricted) is more than 10 in last 5 minutes.&#13;
In the event of a significant increase in denialsâ€”a malicious attacker or misconfigured application could be to blame&#13;
An increase in denied requests will subsequently cause an increase in 403 Forbidden codes. Correlating the two can help you discern the root cause of an increase in 4xx responses.</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Frontend[{#FRONTEND_NAME}]: Request errors per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,ereq]</key><delay>60</delay><description>HTTP request errors per second. &#13;
The frontend request rate measures the number of requests received over the last second. Keeping an eye on peaks and drops is essential to ensure continuous service availability. In the event of a traffic spike, clients could see increases in latency or even denied connections.&#13;
Some of the possible causes are:&#13;
- early termination from the client, before the request has been sent.&#13;
- read error from the client&#13;
- client timeout&#13;
- client closed connection&#13;
- various bad requests from the client.&#13;
- request was tarpitted.</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;10</expression><name>HAProxy Frontend [{#FRONTEND_NAME}]: Average response time is more than 10 sec for 5m</name><priority>AVERAGE</priority><description>Number of request errors in last 5 minutes is more than 10.&#13;
Client-side request errors could have a number of causes:&#13;
    client terminates before sending request&#13;
    read error from client&#13;
    client timeout&#13;
    client terminated connection&#13;
    request was tarpitted/subject to ACL&#13;
Under normal conditions, it is acceptable to (infrequently) receive invalid requests from clients. However, a significant increase in the number of invalid requests received could be a sign of larger, looming issues.&#13;
For example, an abnormal number of terminations or timeouts by numerous clients could mean that your application is experiencing excessive latency, causing clients to manually close their connections.</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Number of responses with codes 1xx per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_1xx]</key><delay>60</delay><description>Number of informational (1xx) HTTP responses per second.</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Number of responses with codes 2xx per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_2xx]</key><delay>60</delay><description>Number of successful HTTP responses per second. ( with 2xx code)</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Number of responses with codes 3xx per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_3xx]</key><delay>60</delay><description>Number of HTTP redirections per second.. ( with 3xx code)</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Number of responses with codes 4xx per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_4xx]</key><delay>60</delay><description>Number of HTTP client errors per second. ( with 4xx code)&#13;
Ideally, all responses forwarded by HAProxy would be class 2xx codes, so an unexpected surge in the number of other code classes could be a sign of trouble.&#13;
Correlating the denial metrics with the response code data can shed light on the cause of an increase in error codes. No change in denials coupled with an increase in the number of 404 responses could point to a misconfigured application or unruly client.</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Number of responses with codes 5xx per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_5xx]</key><delay>60</delay><description>Number of HTTP server errors per second. ( with 5xx code)&#13;
Ideally, all responses forwarded by HAProxy would be class 2xx codes, so an unexpected surge in the number of other code classes could be a sign of trouble.&#13;
Correlating the denial metrics with the response code data can shed light on the cause of an increase in error codes.</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Number of responses with other codes per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_other]</key><delay>60</delay><description>Number of other HTTP server errors per second. ( all other codes, no 1-5xx)</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Sessions rate</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,rate]</key><delay>60</delay><description>Number of sessions created per second&#13;
A significant spike in the number of sessions over a short period could cripple server operations and bring servers down</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes></item_prototype><item_prototype><name>HAProxy Frontend  [{#FRONTEND_NAME}]: Requests rate</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,req_rate]</key><delay>60</delay><description>HTTP requests per second.&#13;
The frontend request rate measures the number of requests received over the last second. Keeping an eye on peaks and drops is essential to ensure continuous service availability. In the event of a traffic spike, clients could see increases in latency or even denied connections.</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Established sessions</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,scur]</key><delay>60</delay><description>The current number of established sessions.</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Session limits</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,slim]</key><delay>60</delay><description>The most simultaneous sessions that are allowed, as defined by the maxconn setting in the frontend.</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes></item_prototype><item_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Session utilization</name><type>CALCULATED</type><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,sutil]</key><delay>60</delay><value_type>FLOAT</value_type><units>%</units><params>last(haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,scur]) / last(haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,slim]) * 100</params><description>Percentage of sessions used (scur / slim * 100).&#13;
 For every HAProxy session, two connections are consumedâ€”one for the client to HAProxy, and the other for HAProxy to your backend.&#13;
Alerting on this metric is essential to ensure your server has sufficient capacity to handle all concurrent sessions. Unlike requests, upon reaching the session limit HAProxy will deny additional clients until resource consumption drops.</description><application_prototypes><application_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]</name></application_prototype></application_prototypes><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;80</expression><name>HAProxy Frontend [{#FRONTEND_NAME}]: Session utilization is more than 80%  for 5m</name><priority>AVERAGE</priority><description>For every HAProxy session, two connections are consumedâ€”one for the client to HAProxy, and the other for HAProxy to your backend. Alerting on this metric is essential to ensure your server has sufficient capacity to handle all concurrent sessions. Unlike requests, upon reaching the session limit HAProxy will deny additional clients until resource consumption drops. Furthermore, if you find your session usage percentage to be hovering above 80%, it could be time to either modify HAProxyâ€™s configuration to allow more sessions, or migrate your HAProxy server to a bigger box.</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype></item_prototypes><graph_prototypes><graph_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Errors and denials per second</name><graph_items><graph_item><color>1A7C11</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,dreq]</key></item></graph_item><graph_item><sortorder>1</sortorder><color>F63100</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,ereq]</key></item></graph_item></graph_items></graph_prototype><graph_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: In/Out traffic</name><graph_items><graph_item><color>2774A4</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,bin]</key></item></graph_item><graph_item><sortorder>1</sortorder><color>A54F10</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,bout]</key></item></graph_item></graph_items></graph_prototype><graph_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Requests and sessions per second</name><graph_items><graph_item><color>2774A4</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,req_rate]</key></item></graph_item><graph_item><sortorder>1</sortorder><color>E64A19</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,slim]</key></item></graph_item><graph_item><sortorder>2</sortorder><color>00FF00</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,rate]</key></item></graph_item></graph_items></graph_prototype><graph_prototype><name>HAProxy Frontend [{#FRONTEND_NAME}]: Responses by HTTP code</name><graph_items><graph_item><color>607D8B</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_1xx]</key></item></graph_item><graph_item><sortorder>1</sortorder><color>4CAF50</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_2xx]</key></item></graph_item><graph_item><sortorder>2</sortorder><color>6C59DC</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_3xx]</key></item></graph_item><graph_item><sortorder>3</sortorder><color>AC8C14</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_4xx]</key></item></graph_item><graph_item><sortorder>4</sortorder><color>611F27</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_5xx]</key></item></graph_item><graph_item><sortorder>5</sortorder><color>BF360C</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#FRONTEND_NAME},FRONTEND,hrsp_other]</key></item></graph_item></graph_items></graph_prototype></graph_prototypes></discovery_rule><discovery_rule><name>HAProxy server discovery</name><key>haproxy.list.discovery[{$HAPROXY_SOCK},SERVER]</key><delay>1h</delay><item_prototypes><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Responses denied per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},dresp]</key><delay>60</delay><status>DISABLED</status><description>Responses denied due to security concerns (ACL-restricted).&#13;
In most cases denials will originate in the frontend (e.g., a user is attempting to access an unauthorized URL). However, sometimes a request may be benign, yet the corresponding response contains sensitive information. In that case, you would want to set up an ACL to deny the offending response. Backend responses that are denied due to ACL restrictions will emit a 502 error code. With properly configured access controls on frontend, this metric should stay at or near zero.&#13;
Denied responses and an increase in 5xx responses go hand-in-hand. If you are seeing a large number of 5xx responses, you should check your denied responses to shed some light on the increase in error codes</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>SIMPLE_CHANGE</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Errors connection per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},econ]</key><delay>60</delay><status>DISABLED</status><description>Number of requests that encountered an error attempting to connect to a backend server.&#13;
Backend connection failures should be acted upon immediately. Unfortunately, the econ metric not only includes failed backend requests but additionally includes general backend errors, like a backend without an active frontend. Thankfully, correlating this metric with eresp and response codes from both frontend and backend servers will give a better idea of the causes of an increase in backend connection errors.</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Response errors per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},eresp]</key><delay>60</delay><status>DISABLED</status><description>Number of requests whose responses yielded an error&#13;
This represents the number of response errors generated by your backends. This includes errors caused by data transfers aborted by the servers as well as write errors on the client socket and failures due to ACLs. Combined with other error metrics, the backend error response rate helps diagnose the root cause of response errors. For example, an increase in both the backend error response rate and denied responses could indicate that clients are repeatedly attempting to access ACL-ed resources.</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;10</expression><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Number of responses with error is more than 10s for 5m</name><priority>AVERAGE</priority><description>Number of requests on server, whose responses yielded an error, is more than 10.&#13;
The server error response rate represents the number of response errors generated by your servers. This includes errors caused by data transfers aborted by the servers as well as write errors on the client socket and failures due to ACLs. Combined with other error metrics, the server error response rate helps diagnose the root cause of response errors. For example, an increase in both the server error response rate and denied responses could indicate that clients are repeatedly attempting to access ACL-ed resources.</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Number of responses with codes 4xx per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},hrsp_4xx]</key><delay>60</delay><status>DISABLED</status><description>Number of HTTP client errors per second.</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Number of responses with codes 5xx per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},hrsp_5xx]</key><delay>60</delay><status>DISABLED</status><description>Number of HTTP server errors per second.</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Unassigned requests</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},qcur]</key><delay>60</delay><status>DISABLED</status><description>Current number of requests unassigned in queue.&#13;
The qcur metric tracks the current number of connections awaiting assignment to a backend server. If you have enabled cookies and the listed server is unavailable, connections will be queued until the queue timeout is reached</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;10</expression><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Current number of requests unassigned in queue is more than 10s for 5m</name><priority>AVERAGE</priority><description>Current number of requests unassigned in queue is more than 10.&#13;
If your server is bombarded with connections to the point you have reached your global maxconn limit, HAProxy will seamlessly queue new connections in system kernelâ€™s socket queue until the server becomes available.&#13;
Keeping connections out of the queue is ideal, resulting in less latency and a better user experience. You should alert if the size of your queue exceeds the threshold. If you find that connections are consistently enqueueing, configuration changes may be in order, such as increasing global maxconn limit or changing the connection limits on individual backend servers.&#13;
Keep in mind: empty queue = happy client</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Time in queue</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},qtime]</key><delay>60</delay><status>DISABLED</status><value_type>FLOAT</value_type><units>s</units><description>Average time spent in queue (in ms) for the last 1,024 requests&#13;
Minimizing time spent in the queue results in lower latency and an overall better client experience. Each use case can tolerate a certain amount of queue time but in general, you should aim to keep this value as low as possible</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>MULTIPLIER</type><params>0.001</params></step></preprocessing><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;10s</expression><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Average time spent in queue is more than 10s for 5m</name><priority>AVERAGE</priority><description>Average time spent in queue (in ms) for the last 1,024 requests is more than 10s.&#13;
It is obviously that minimizing time spent in the queue results in lower latency and an overall better client experience. Each use case can tolerate a certain amount of queue time but in general, you should aim to keep this value as low as possible</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Responses time</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},rtime]</key><delay>60</delay><status>DISABLED</status><value_type>FLOAT</value_type><units>s</units><description>Average backend response time (in ms) for the last 1,024 requests&#13;
Tracking average response times is an effective way to measure the latency of your load-balancing setup. Generally speaking, response times in excess of 500 ms will lead to degradation of application performance and customer experience. Monitoring the average response time can give you the upper hand to respond to latency issues before your customers are substantially impacted.&#13;
Keep in mind that this metric will be zero if you are not using HTTP (see #60)</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>MULTIPLIER</type><params>0.001</params></step></preprocessing><trigger_prototypes><trigger_prototype><expression>{min(5m)}&gt;10s</expression><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Average response time is more than 10s for 5m</name><priority>AVERAGE</priority><description>Average server response time (in ms) for the last 1,024 requests is more than 10s.&#13;
Tracking average response times is an effective way to measure the latency of haproxy load-balancing setup. Generally speaking, response times in excess of 500 ms will lead to degradation of application performance and customer experience. Monitoring the average response time can give you the upper hand to respond to latency issues before your customers are substantially impacted.&#13;
Keep in mind that this metric will be zero if you are not using HTTP</description><tags><tag><tag>Value</tag><value>{ITEM.VALUE}</value></tag></tags></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Status</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},status]</key><delay>60</delay><description>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}] status&#13;
UP = 1&#13;
DOWN = 0</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><valuemap><name>Service state</name></valuemap><preprocessing><step><type>BOOL_TO_DECIMAL</type><params/></step></preprocessing><trigger_prototypes><trigger_prototype><expression>{max(#5)}=0</expression><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Server is DOWN</name><priority>DISASTER</priority><description>Server is not available.&#13;
The check directive must be enabled in HAProxy server configuration</description></trigger_prototype></trigger_prototypes></item_prototype><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Redispatched requests per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},wredis]</key><delay>60</delay><status>DISABLED</status><description>Number of times a request was redispatched to a different backend.&#13;
The redispatch rate metric tracks the number of times a client connection was unable to reach its original target, and was subsequently sent to a different server. If a client holds a cookie referencing a backend server that is down, the default action is to respond to the client with a 502 status code. However, if is  enabled option redispatch in haproxy.cfg, the request will be sent to any available backend server and the cookie will be ignored.</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype><item_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}]: Retried connections per second</name><type>ZABBIX_ACTIVE</type><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},wretr]</key><delay>60</delay><status>DISABLED</status><description>Number of times a connection was retried.&#13;
Some dropped or timed-out connections are to be expected when connecting to a backend server. The retry rate represents the number of times a connection to a backend server was retried. This metric is usually non-zero under normal operating conditions. Should you begin to see more retries than usual, it is likely that other metrics will also change, including econ and eresp.&#13;
Tracking the retry rate in addition to the above two error metrics can shine some light on the true cause of an increase in errors</description><application_prototypes><application_prototype><name>HAProxy Backend [{#BACKEND_NAME}]</name></application_prototype></application_prototypes><preprocessing><step><type>CHANGE_PER_SECOND</type><params/></step></preprocessing></item_prototype></item_prototypes><graph_prototypes><graph_prototype><name>HAProxy Server [{#BACKEND_NAME}/{#SERVER_NAME}] Response time and time in queue</name><percent_right>95</percent_right><graph_items><graph_item><color>388E3C</color><yaxisside>RIGHT</yaxisside><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},qtime]</key></item></graph_item><graph_item><sortorder>1</sortorder><color>F63100</color><item><host>HAProxy</host><key>haproxy.stats[{$HAPROXY_SOCK},{#BACKEND_NAME},{#SERVER_NAME},rtime]</key></item></graph_item></graph_items></graph_prototype></graph_prototypes></discovery_rule></discovery_rules><macros><macro><macro>{$HAPROXY_CONFIG}</macro><value>/etc/haproxy/haproxy.cfg</value></macro><macro><macro>{$HAPROXY_SOCK}</macro><value>/var/lib/haproxy/stats</value></macro></macros></template></templates><value_maps><value_map><name>Service state</name><mappings><mapping><value>0</value><newvalue>Down</newvalue></mapping><mapping><value>1</value><newvalue>Up</newvalue></mapping></mappings></value_map></value_maps></zabbix_export>
